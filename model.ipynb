{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "POhDNuHnd-a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.4.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
            "    app.start()\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 758, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
            "    handle._run()\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n",
            "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n",
            "    await result\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n",
            "    await super().execute_request(stream, ident, parent)\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3123, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3178, in _run_cell\n",
            "    result = runner(coro)\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3641, in run_ast_nodes\n",
            "    if await self.run_code(code, result, async_=asy):\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3701, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/var/folders/by/5g5x85y535329m06mk5cbqsh0000gn/T/ipykernel_2122/3203824528.py\", line 4, in <module>\n",
            "    import torch\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/opt/anaconda3/envs/pytorch_cpu/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     /Users/imtiaz/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/imtiaz/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/imtiaz/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBiwHl_QeK3T"
      },
      "source": [
        "The nltk.corpus.gutenberg module is a built-in dataset within the Natural Language Toolkit (NLTK) library for Python. It provides a curated selection of roughly 10-15 classic, public-domain books from the Project Gutenberg electronic text archive, totaling approximately 2 million words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb1XTQxveIpH"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import gutenberg\n",
        "print(gutenberg.fileids())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Pdi0QXJeIr8"
      },
      "outputs": [],
      "source": [
        "# For limitation of resources, we are using only one book from gutenberg archive\n",
        "corpus = gutenberg.raw('shakespeare-hamlet.txt')\n",
        "\n",
        "# Clean the corpus: Remove metadata, stage directions, speaker names\n",
        "corpus = re.sub(r'\\[.*?\\]', '', corpus)  # Remove brackets\n",
        "corpus = re.sub(r'Actus.*?\\.', '', corpus, flags=re.DOTALL)  # Remove act/scene headers\n",
        "corpus = re.sub(r'Scena.*?\\.', '', corpus, flags=re.DOTALL)\n",
        "corpus = re.sub(r'Enter.*?\\.', '', corpus, flags=re.DOTALL)  # Remove enter/exit\n",
        "corpus = re.sub(r'Exit.*?\\.', '', corpus, flags=re.DOTALL)\n",
        "corpus = re.sub(r'Exeunt.*?\\.', '', corpus, flags=re.DOTALL)\n",
        "corpus = re.sub(r'\\w+\\.', '', corpus)  # Remove speaker names like 'Ham.'\n",
        "corpus = re.sub(r'\\s+', ' ', corpus).strip()  # Normalize spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus[0:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# total words\n",
        "print(len(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize corpus and add sentence boundaries\n",
        "corpus = corpus.lower().replace(\"\\n\", \" <eos> \")\n",
        "tokens = word_tokenize(corpus)\n",
        "\n",
        "\n",
        "# Build vocabulary\n",
        "counter = Counter(tokens)\n",
        "\n",
        "# special tokens\n",
        "specials = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",
        "\n",
        "# keep words appearing at least twice\n",
        "vocab_list = specials + [w for w, c in counter.items() if c >= 2 and w not in specials]\n",
        "# Maps between words and indices\n",
        "word2idx = {w: i for i, w in enumerate(vocab_list)}\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "\n",
        "pad_idx = word2idx[\"<pad>\"]\n",
        "unk_idx = word2idx[\"<unk>\"]\n",
        "\n",
        "\n",
        "# Any word not in vocab becomes <unk>\n",
        "numerical_tokens = [word2idx.get(tok, unk_idx) for tok in tokens]\n",
        "\n",
        "print(f\"Vocab size: {len(word2idx)} | Total tokens: {len(numerical_tokens)}\")\n",
        "\n",
        "\n",
        "# Create sliding windows\n",
        "def create_sequences(tokens, seq_len):\n",
        "    inputs, targets = [], []\n",
        "    for i in range(len(tokens) - seq_len):\n",
        "        # input window\n",
        "        inputs.append(tokens[i : i + seq_len])\n",
        "        # next-token targets\n",
        "        targets.append(tokens[i + 1 : i + seq_len + 1])\n",
        "    return torch.tensor(inputs), torch.tensor(targets)\n",
        "\n",
        "\n",
        "SEQ_LEN = 50\n",
        "inputs, targets = create_sequences(numerical_tokens, SEQ_LEN)\n",
        "\n",
        "print(\"Total sequences:\", len(inputs))\n",
        "# Split sequences into train / val / test\n",
        "n = len(inputs)\n",
        "train_end = int(0.85 * n)\n",
        "val_end = int(0.95 * n)\n",
        "\n",
        "# slice\n",
        "train_inputs = inputs[:train_end]\n",
        "train_targets = targets[:train_end]\n",
        "\n",
        "val_inputs = inputs[train_end:val_end]\n",
        "val_targets = targets[train_end:val_end]\n",
        "\n",
        "test_inputs = inputs[val_end:]\n",
        "test_targets = targets[val_end:]\n",
        "\n",
        "print(\"Train:\", train_inputs.shape)\n",
        "print(\"Val:  \", val_inputs.shape)\n",
        "print(\"Test: \", test_inputs.shape)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    TensorDataset(train_inputs, train_targets), batch_size=32, shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    TensorDataset(val_inputs, val_targets), batch_size=32, shuffle=False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    TensorDataset(test_inputs, test_targets), batch_size=32, shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0pKlH-UeeY_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqKjTAxPehFj"
      },
      "source": [
        "# Model Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Long Short-Term Memory (LSTM) networks are gated recurrent neural networks that learn long-range dependencies by keeping a dedicated *cell state* (c_t) and controlling information flow with gates. This architecture helps gradients survive across many time steps.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Core equations (standard LSTM)\n",
        "\n",
        "Notation:\n",
        "\n",
        "* (x_t\\in\\mathbb{R}^d) — input at time (t)\n",
        "* (h_t\\in\\mathbb{R}^h) — hidden state / output at time (t)\n",
        "* (c_t\\in\\mathbb{R}^h) — cell state at time (t)\n",
        "* (W_\\bullet\\in\\mathbb{R}^{h\\times d},;U_\\bullet\\in\\mathbb{R}^{h\\times h},;b_\\bullet\\in\\mathbb{R}^h) — parameters\n",
        "\n",
        "At each step:\n",
        "[\n",
        "\\begin{aligned}\n",
        "i_t &= \\sigma(W_i x_t + U_i h_{t-1} + b_i) \\\n",
        "f_t &= \\sigma(W_f x_t + U_f h_{t-1} + b_f) \\\n",
        "\\tilde{c}*t &= \\tanh(W_c x_t + U_c h*{t-1} + b_c) \\\n",
        "c_t &= f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}*t \\\n",
        "o_t &= \\sigma(W_o x_t + U_o h*{t-1} + b_o) \\\n",
        "h_t &= o_t \\odot \\tanh(c_t)\n",
        "\\end{aligned}\n",
        "]\n",
        "\n",
        "* (\\sigma(\\cdot)) is the sigmoid (0..1); (\\tanh(\\cdot)) is used for the candidate and final squashing.\n",
        "* (\\odot) denotes elementwise product.\n",
        "\n",
        "**Compact (concatenated) form:** put the four gate pre-activations into one vector:\n",
        "[\n",
        "z_t = W x_t + U h_{t-1} + b \\in\\mathbb{R}^{4h},\n",
        "]\n",
        "then split (z_t) into ([i_{pre}, f_{pre}, o_{pre}, g_{pre}]) and apply activations to recover (i_t,f_t,o_t,\\tilde c_t). This is what most frameworks do for efficiency.\n",
        "\n",
        "---\n",
        "\n",
        "## Intuition (what each gate does)\n",
        "\n",
        "* **Forget gate (f_t)**: how much of past cell (c_{t-1}) to keep.\n",
        "* **Input gate (i_t)**: how much new information (\\tilde c_t) to write into the cell.\n",
        "* **Cell (c_t)**: long-term memory that accumulates retained past + new info.\n",
        "* **Output gate (o_t)**: how much of the cell (after (\\tanh)) to expose as hidden state (h_t).\n",
        "\n",
        "The gates let the model *learn* to remember or forget over long ranges, which mitigates vanishing gradients.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u6AS32SNehjt"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        # LSTM with dropout between layers (only works if num_layers > 1)\n",
        "        self.lstm = nn.LSTM(\n",
        "            embed_size,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "        )\n",
        "\n",
        "        # Dropout on LSTM outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Final classifier\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T)\n",
        "        x = self.embedding(x)  # (B, T, E)\n",
        "        out, _ = self.lstm(x)  # (B, T, H)\n",
        "        out = self.dropout(out)  # regularization\n",
        "        out = self.fc(out)  # (B, T, V)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0RlN07_ertt"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LSTMModel(\n",
        "    vocab_size=len(word2idx),\n",
        "    embed_size=256,\n",
        "    hidden_size=256,\n",
        "    num_layers=2,\n",
        "    dropout=0.3,  \n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-5) \n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=word2idx[\"<pad>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xb62bfDDetnX"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga1ElPKbevXt"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_loss_and_ppl(model, loader, vocab_size, pad_idx, device):\n",
        "    \"\"\"\n",
        "    Calculate loss and Perplexity\n",
        "    \"\"\"\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx, reduction=\"sum\")\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad(): # disable grad\n",
        "        for input_batch, target_batch in loader:\n",
        "            input_batch = input_batch.to(device)\n",
        "            target_batch = target_batch.to(device)\n",
        "\n",
        "            logits = model(input_batch)  # (B, T, V)\n",
        "\n",
        "            loss = criterion(\n",
        "                logits.view(-1, vocab_size),\n",
        "                target_batch.view(-1)\n",
        "            ) # Token-level negative log-likelihood (sum)\n",
        "\n",
        "            # Count non-pad tokens\n",
        "            total_loss += loss.item()\n",
        "            total_tokens += (target_batch != pad_idx).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    ppl = math.exp(avg_loss)\n",
        "\n",
        "    if was_training:\n",
        "        model.train()\n",
        "\n",
        "    return avg_loss, ppl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vZvEe9Fevsy"
      },
      "outputs": [],
      "source": [
        "def train_model(epochs, train_loader, val_loader, pad_idx):\n",
        "    # to store each epoch losses\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_ppls = []\n",
        "    val_ppls = []\n",
        "\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # train \n",
        "        model.train()\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            input_batch = input_batch.to(device)\n",
        "            target_batch = target_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad() # Clear old gradients\n",
        "\n",
        "            logits = model(input_batch)  # (B, T, V)\n",
        "\n",
        "            loss = criterion(\n",
        "                logits.view(-1, len(word2idx)),\n",
        "                target_batch.view(-1)\n",
        "            ) # Compute token-level cross-entropy\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clip gradients to prevent exploding gradients\n",
        "            optimizer.step()\n",
        "\n",
        "        # eval (no grad)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            #  compute TRAIN loss & ppl\n",
        "            train_loss, train_ppl = calc_loss_and_ppl(\n",
        "                model,\n",
        "                train_loader,\n",
        "                vocab_size=len(word2idx),\n",
        "                pad_idx=pad_idx,\n",
        "                device=device\n",
        "            )\n",
        "            # compute VAL loss & ppl \n",
        "            val_loss, val_ppl = calc_loss_and_ppl(\n",
        "                model,\n",
        "                val_loader,\n",
        "                vocab_size=len(word2idx),\n",
        "                pad_idx=pad_idx,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_ppls.append(train_ppl)\n",
        "        val_losses.append(val_loss)\n",
        "        val_ppls.append(val_ppl)\n",
        "\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            print(\n",
        "                f\"Epoch {epoch+1:3d}/{epochs} | \"\n",
        "                f\"Train Loss: {train_loss:.2f} | \"\n",
        "                f\"Train PPL: {train_ppl:.2f} | \"\n",
        "                f\"Val Loss: {val_loss:.2f} | \"\n",
        "                f\"Val PPL: {val_ppl:.2f}\"\n",
        "            )\n",
        "\n",
        "    total_time = time.perf_counter() - start_time\n",
        "    return train_losses, val_losses, train_ppls, val_ppls, total_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUH_I8sxfK0T"
      },
      "outputs": [],
      "source": [
        "num_epoches = 10\n",
        "res = train_model(\n",
        "    epochs=num_epoches,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    pad_idx=word2idx[\"<pad>\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"tr.png\"  width=\"500\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\n",
        "    f\"{'Epoch':<15}{'Train loss':<15}{'Train PPL':<15}{'Val loss':<15}{'Val PPL':<15}{'Training time(ms)'}\"\n",
        ")\n",
        "print(\n",
        "    f\"{num_epoches:<15}{sum(res[0])/num_epoches:<15.2f}{sum(res[2])/num_epoches:<15.2f}{sum(res[1])/num_epoches:<15.2f}{sum(res[-2])/num_epoches:<15.2f}{res[-1]* 1000:.2f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"fl.png\"  width=\"500\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test\n",
        "test_loss, test_ppl = calc_loss_and_ppl(\n",
        "    model, test_loader, vocab_size=len(word2idx), pad_idx=pad_idx, device=device\n",
        ")\n",
        "print(f\"{'Test loss':<15}{'Test PPL'}\")\n",
        "print(f\"{test_loss:<15.2f}{test_ppl:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"tl.png\"  width=\"500\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx4K24ulfL15"
      },
      "source": [
        "# Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "saved_path = os.path.join(os.getcwd(), \"app\", \"saved_model\")\n",
        "torch.save({\n",
        "    \"model_state\": model.state_dict(),\n",
        "    \"optimizer_state\": optimizer.state_dict(),\n",
        "    \"word2idx\": word2idx,\n",
        "    \"idx2word\": idx2word,\n",
        "}, os.path.join(saved_path, \"checkpoint1.pt\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (embedding): Embedding(1745, 256)\n",
              "  (lstm): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (fc): Linear(in_features=256, out_features=1745, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "saved_path = os.path.join(os.getcwd(), \"app\", \"saved_model\")\n",
        "ckpt = torch.load(os.path.join(saved_path, \"checkpoint.pt\"), map_location=device)\n",
        "\n",
        "model = LSTMModel(\n",
        "    vocab_size=len(ckpt[\"word2idx\"]),\n",
        "    embed_size=256,\n",
        "    hidden_size=256,\n",
        "    num_layers=2,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=3e-4,          \n",
        "    weight_decay=1e-5\n",
        ")\n",
        "\n",
        "\n",
        "model.load_state_dict(ckpt[\"model_state\"])\n",
        "optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
        "\n",
        "word2idx = ckpt[\"word2idx\"]\n",
        "idx2word = ckpt[\"idx2word\"]\n",
        "\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-abn4lvZfLPa"
      },
      "outputs": [],
      "source": [
        "def generate_text(\n",
        "    model, word2idx, idx2word, seed_text,\n",
        "    length=50, temperature=0.8, sequence_length=50\n",
        "):\n",
        "    model.eval()\n",
        "\n",
        "    tokens = word_tokenize(seed_text.lower())\n",
        "    indices = [word2idx.get(token, word2idx[\"<unk>\"]) for token in tokens]\n",
        "    generated = indices.copy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            current_seq = generated[-sequence_length:]\n",
        "\n",
        "            if len(current_seq) < sequence_length:\n",
        "                current_seq = [word2idx[\"<pad>\"]] * (sequence_length - len(current_seq)) + current_seq\n",
        "\n",
        "            input_tensor = torch.tensor([current_seq], dtype=torch.long).to(device)\n",
        "\n",
        "            output = model(input_tensor)\n",
        "\n",
        "            logits = output[0, -1] / temperature\n",
        "            probs = torch.softmax(logits, dim=0)\n",
        "            next_token = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            generated.append(next_token)\n",
        "\n",
        "    return \" \".join(idx2word.get(idx, \"<unk>\") for idx in generated)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1Pxw1n7pfMwK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5\n",
            "--------------------\n",
            "Generated sequence from \"he was\":\n",
            "he was but for i haue seene <unk> , as 'twere may go the <unk> through the time of my defence but of all youth to earth\n",
            "\n",
            "\n",
            "0.6\n",
            "--------------------\n",
            "Generated sequence from \"he was\":\n",
            "he was a guildensterne ? now , i could the <unk> <unk> : for so capitall i <unk> my husband flye to : but that she speakes\n",
            "\n",
            "\n",
            "0.7\n",
            "--------------------\n",
            "Generated sequence from \"he was\":\n",
            "he was for your good polonius good friends , and may ( not <unk> i know my <unk> , let want <unk> note , and you do\n",
            "\n",
            "\n",
            "0.8\n",
            "--------------------\n",
            "Generated sequence from \"he was\":\n",
            "he was by a great lose , make <unk> the first : he comes to the <unk> , a <unk> vulgar , the pardon that was a\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "seed = \"he was\"\n",
        "for i in [0.5,0.6,0.7,0.8]:\n",
        "    print(i)\n",
        "    print(\"--\"*10)\n",
        "    generated_sequence = generate_text(model, word2idx, idx2word, seed,temperature=i, length=25)\n",
        "    print(f'Generated sequence from \"{seed}\":\\n{generated_sequence}')\n",
        "    print('\\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch_cpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
